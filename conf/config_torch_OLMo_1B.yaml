# This argument is for OLMo (https://github.com/allenai/OLMo/blob/main/olmo/config.py#L234)
# WARNING
# 1. n_positions & n_ctx should be +1(because of label) than train_length
model:
  _name_or_path: "allenai/OLMo-1B-hf"
  config:
    d_model: 2048
    n_heads: 16
    n_layers: 16
    mlp_ratio: 8
    weight_tying: true
    alibi: false
    rope: true
    flash_attention: false  # not available on AMD
    attention_dropout: 0.0
    attention_layer_norm: false
    multi_query_attention: false
    include_bias: false
    block_type: sequential
    layer_norm_type: default
    layer_norm_with_affine: false
    bias_for_layer_norm: false
    attention_layer_norm_with_affine: false
    activation_type: swiglu
    residual_dropout: 0.0
    embedding_dropout: 0.0
    max_sequence_length: 2048
    vocab_size: 50280
    embedding_size: 50304
    eos_token_id: 50279
    pad_token_id: 1
    init_device: meta
    init_fn: mitchell

dataset:
  train_length: 10

optimizer:
  optimizer_name: AdamW

train:
  seed: 42
  wandb: True
  wandb_project_name: confounder_analysis
  wandb_entity: lklab_kaist
  batch_size: 512
  learning_rate: 1e-5
  weight_decay: 
  momentum: 
  warmup_steps: 1
  num_epochs: 3
  max_grad_norm: 