# This argument is for GPT2Model (https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py#L30)
# WARNING
# 1. n_positions & n_ctx should be +1(because of label) than train_length
defaults:
  - model: gpt2_small


dataset:
  train_length: 10
  max_data_num: 100000
  noise_ratio: 0.2
  skip_train_noisy:
  skip_train_special_code: 
  only_train_noisy:
  only_train_special_code: 

optimizer:
  optimizer_name: AdamW
  learning_rate: 1e-4
  weight_decay: 
  momentum: 

train:
  seed: 42
  wandb: True
  wandb_project_name: confounder_analysis
  wandb_entity: lklab_kaist
  batch_size: 128
  warmup_steps: 0
  num_epochs: 16
  max_grad_norm: 
  val_interval: 4
  save_model_interval: 4