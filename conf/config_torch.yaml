
defaults:
  - model: L1_H1_E16.yaml

dataset:
  type: alphabet
  train_length: 6
  num_data: 10000
  # max_data_num: 100000
  # max_alphabet_data_num: 10
  # noise_ratio: [0.0, 0.0, 0.0]
  # special_code: '10'
  copy_pos: 0
  train_alphabets: ['a', 'b', 'c']
  val_alphabets: ['a', 'b', 'c', 'd']
  answer_ratio: [0.6, 0.3, 0.1]
  # skip_train_noisy:
  # skip_train_special_code: 
  # only_train_noisy:
  # only_train_special_code: 
  # general: [True, False, True]
  # only_special_code: [False, True, False]
  # only_noise: [False, False, False]
  # noisy_special_code: [False, False, False]

optimizer:
  optimizer_name: AdamW
  learning_rate: 1e-4
  weight_decay: 1
  # momentum: 0.0

train:
  # exp_name: L${model.n_layer}_H${model.n_head}_E${model.n_embd}_len${dataset.train_length}_code${dataset.special_code}_pos${dataset.copy_pos}_bsize${train.batch_size}_lr${optimizer.learning_rate}_wd${optimizer.weight_decay}_inject_special
  exp_name: L${model.n_layer}_H${model.n_head}_E${model.n_embd}_len${dataset.train_length}_t${dataset.train_alphabets}_v${dataset.val_alphabets}_r${dataset.answer_ratio}_pos${dataset.copy_pos}_bsize${train.batch_size}_lr${optimizer.learning_rate}_wd${optimizer.weight_decay}_${freezed_params:${train.freeze_word_emb},${train.freeze_pos_emb},${train.train_attn_only}}
  # exp_name: alphabet-debug
  seed: 42
  wandb: True
  wandb_project_name: alphabet_copy
  wandb_entity: lklab_kaist
  batch_size: 64
  warmup_steps: 0
  config_steps: [100000]
  max_grad_norm: 
  val_interval: 100
  save_model_interval: 100
  log_correct: True
  freeze_word_emb: false
  freeze_pos_emb: false
  train_attn_only: true