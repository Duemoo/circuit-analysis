# This argument is for GPT2Model
# WARNING
# 1. n_positions & n_ctx should be +1(because of label) than train_length
model:
  _name_or_path: openai-community/gpt2
  vocab_size: 2
  n_positions: 11
  n_ctx: 11
  n_embd: 768
  n_layer: 12
  n_head: 12
  n_inner:
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  layer_norm_epsilon: 1e-5
  initializer_range: 0.02
  summary_type: cls_index
  summary_use_proj: True
  summary_activation: 
  summary_proj_to_labels: True
  summary_first_dropout: 0.1
  scale_attn_weights: True
  use_cache: True
  bos_token_id: 50256
  eos_token_id: 50256
  scale_attn_by_inverse_layer_idx: False
  reorder_and_upcast_attn: False

dataset:
  train_length: 10

optimizer:
  optimizer_name: AdamW

train:
  seed: 42
  wandb: True
  wandb_project_name: confounder_analysis
  wandb_entity: lklab_kaist
  batch_size: 512
  learning_rate: 1e-5
  weight_decay: 
  momentum: 
  warmup_steps: 1
  num_epochs: 3
  max_grad_norm: 