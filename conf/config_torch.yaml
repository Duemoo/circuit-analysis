
defaults:
  - model: gpt2_small


dataset:
  train_length: 10
  max_data_num: 100000
  noise_ratio: [0.0, 0.0, 0.5, 0.5]
  # skip_train_noisy:
  # skip_train_special_code: 
  # only_train_noisy:
  # only_train_special_code: 
  general: [True, True, False, True]
  only_special_code: [True, True, False, False]
  only_noise: [True, False, True, False]
  noisy_special_code: [True, False, True, False]

optimizer:
  optimizer_name: AdamW
  learning_rate: 1e-4
  weight_decay: 
  momentum: 

train:
  seed: 42
  wandb: True
  wandb_project_name: confounder_analysis
  wandb_entity: lklab_kaist
  batch_size: 128
  warmup_steps: 0
  num_epochs: 16
  max_grad_norm: 
  val_interval: 4
  save_model_interval: 4