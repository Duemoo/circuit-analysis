# This argument is for GPT2Model (https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py#L30)

_name_or_path: openai-community/gpt2
activation_function: gelu_new
# The dropout ratio for the attention
attn_pdrop: 0.1
bos_token_id: 9999
# The dropout ratio for the embeddings
embd_pdrop: 0.1
eos_token_id: 9999
initializer_range: 0.02
# The epsilon to use in the layer normalization layers
layer_norm_epsilon: 1e-05
n_ctx: ${eval:'${dataset.train_length} + 10'}
n_embd: 64
n_head: 1
n_layer: 1
n_positions: ${eval:'${dataset.train_length} + 10'}
# The dropout probability for all fully connected layers in the embeddings, encoder, and pooler
resid_pdrop: 0.1
summary_activation:
summary_first_dropout: 0.1
summary_proj_to_labels: True
summary_type: cls_index
summary_use_proj: True
vocab_size: 27