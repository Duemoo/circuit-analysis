{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU-1GTZT28li"
      },
      "source": [
        "# The tuned lens 🔎\n",
        "A tuned lens allows us to peak at the iterative computations that a transformer is using the compute the next token.\n",
        "\n",
        "A lens into a transformer with n layers allows you to replace the last $m$ layers of the model with an [affine transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (we call these affine translators).\n",
        "\n",
        "This essentially skips over these last few layers and lets you see the best prediction that can be made from the model's representations, i.e. the residual stream, at layer $n - m$. Since the representations may be rotated, shifted, or stretched from layer to layer it's useful to train the len's affine translators specifically on each layer. This training is what differentiates this method from simpler approaches that decode the residual stream of the network directly using the unembedding layer i.e. the logit lens. We explain this process along with more applications of the method in [the paper](ttps://arxiv.org/abs/2303.08112).\n",
        "\n",
        "You can find the complete set of pretrained lenses on [the hugging face space](https://huggingface.co/spaces/AlignmentResearch/tuned-lens/tree/main/lens).\n",
        "\n",
        "## Usage\n",
        "Since the tuned lens produces a distribution of predictions to visualize it's output we need to we need to provide a summary statistic to plot.  The default is simply [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)), but you can also choose the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) with the target token, or the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the model's predictions and the tuned lens' predictions. You can also hover over a token to see more of the distribution i.e. the top 10 most probable tokens and their probabilities.\n",
        "\n",
        "## Examples\n",
        "Some interesting examples you can try.\n",
        "\n",
        "### Copy paste:\n",
        "```\n",
        "Copy: A!2j!#u&NGApS&MkkHe8Gm!#\n",
        "Paste: A!2j!#u&NGApS&MkkHe8Gm!#\n",
        "```\n",
        "\n",
        "### Trivial in-context learning\n",
        "```\n",
        "inc 1 2\n",
        "inc 4 5\n",
        "inc 13\n",
        "```\n",
        "\n",
        "#### Addition\n",
        "```\n",
        "add 1 1 2\n",
        "add 3 4 7\n",
        "add 13 2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W9cRsIdK2-Jm",
        "outputId": "b339adf5-3a49-453c-8625-24b792d4d466"
      },
      "outputs": [],
      "source": [
        "# !pip install tuned-lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from tuned_lens.nn.lenses import TunedLens, LogitLens\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(dotenv_path=\"./.env\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current GPU num: 1\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Current GPU num: {torch.cuda.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMPORTANT!! Load Trained Model & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_dir = os.getenv(\"MODEL_SAVE_PATH\", \"checkpoints\")\n",
        "exp_name = 'L2_H2_E16_len20_code101_bsize1_lr0.0001_wd0.5_pure-mixed'\n",
        "step = 0\n",
        "checkpoint_path = os.path.join(checkpoint_dir, exp_name, f\"checkpoint-{step}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model: GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(3, 16)\n",
            "    (wpe): Embedding(21, 16)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-1): 2 x GPT2Block(\n",
            "        (ln_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=16, out_features=3, bias=False)\n",
            ")\n",
            "/mnt/nas/jinho/saved_checkpoints/L2_H2_E16_len20_code101_bsize1_lr0.0001_wd0.5_pure-mixed/checkpoint-0\n",
            "L2_H2_E16_len20_code101_bsize1_lr0.0001_wd0.5_pure-mixed/checkpoint-0\n",
            "Loaded tokenizer: GPT2TokenizerFast(name_or_path='/mnt/nas/jinho/saved_checkpoints/L2_H2_E16_len20_code101_bsize1_lr0.0001_wd0.5_pure-mixed/checkpoint-0', vocab_size=2, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t2: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# To try a diffrent modle / lens check if the lens is avalible then modify this code\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
        "print(f\"Loaded model: {model}\")\n",
        "print(model.config.name_or_path)\n",
        "model_name = model.name_or_path if len(model.name_or_path.split(\"/\")) < 2 else \"/\".join(model.name_or_path.split(\"/\")[-2:])\n",
        "print(model_name)\n",
        "model = model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "print(f\"Loaded tokenizer: {tokenizer}\")\n",
        "# tuned_lens = TunedLens.from_model_and_pretrained(model)\n",
        "# tuned_lens = tuned_lens.to(device)\n",
        "logit_lens = LogitLens.from_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FGd2YmyD28lk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# To try a diffrent modle / lens check if the lens is avalible then modify this code\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
        "# print(f\"Loaded model: {model}\")\n",
        "# print(model.config.name_or_path)\n",
        "# model = model.to(device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
        "# print(f\"Loaded tokenizer: {tokenizer}\")\n",
        "# tuned_lens = TunedLens.from_model_and_pretrained(model)\n",
        "# tuned_lens = tuned_lens.to(device)\n",
        "# logit_lens = LogitLens.from_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z9AhuLaDBDRN"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "67c815c5b3654ed7b2fe2792c1a61d0c"
          ]
        },
        "id": "DL8f4i2828lm",
        "outputId": "3ad90e78-3e4d-4b45-d199-c15b2dd1d785",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "309f296946b14907b73c54e615f5a61c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Select Lens:', options=(('Logit Lens', LogitLens(\n",
              "  (unembed): Une…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tuned_lens.plotting import PredictionTrajectory\n",
        "import ipywidgets as widgets\n",
        "from plotly import graph_objects as go\n",
        "\n",
        "\n",
        "def make_plot(lens, text, layer_stride, statistic, token_range):\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    targets = input_ids[1:] + [tokenizer.eos_token_id]\n",
        "\n",
        "    if len(input_ids) == 0:\n",
        "        return widgets.Text(\"Please enter some text.\")\n",
        "\n",
        "    if (token_range[0] == token_range[1]):\n",
        "        return widgets.Text(\"Please provide valid token range.\")\n",
        "    pred_traj = PredictionTrajectory.from_lens_and_model(\n",
        "        lens=lens,\n",
        "        model=model,\n",
        "        input_ids=input_ids,\n",
        "        tokenizer=tokenizer,\n",
        "        targets=targets,\n",
        "    ).slice_sequence(slice(*token_range))\n",
        "\n",
        "    # .stride() : 첫번째 layer부터 몇 개의 layer 간격으로 output을 뽑을 건가?\n",
        "    # 만약 stride가 3이고 layer가 총 0~9까지 총 10개 있었다면, 선별되는 layer는 0, 3, 6, 9, 10이 됨!\n",
        "    return getattr(pred_traj, statistic)(topk=3).stride(layer_stride).figure(\n",
        "        title=f\"{lens.__class__.__name__} ({model_name}) {statistic}\",\n",
        "    )\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "statistic_wdg = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Entropy', 'entropy'),\n",
        "        ('Cross Entropy', 'cross_entropy'),\n",
        "        ('Forward KL', 'forward_kl'),\n",
        "    ],\n",
        "    description='Select Statistic:',\n",
        "    style=style,\n",
        ")\n",
        "text_wdg = widgets.Textarea(\n",
        "    description=\"Input Text\",\n",
        "    value=\"1100000100\",\n",
        ")\n",
        "# lens_wdg = widgets.Dropdown(\n",
        "#     options=[('Tuned Lens', tuned_lens), ('Logit Lens', logit_lens)],\n",
        "#     description='Select Lens:',\n",
        "#     style=style,\n",
        "# )\n",
        "lens_wdg = widgets.Dropdown(\n",
        "    options=[('Logit Lens', logit_lens)],\n",
        "    description='Select Lens:',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "layer_stride_wdg = widgets.BoundedIntText(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    step=1,\n",
        "    description='Layer Stride:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "token_range_wdg = widgets.IntRangeSlider(\n",
        "    description='Token Range',\n",
        "    min=0,\n",
        "    max=1,\n",
        "    step=1,\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "\n",
        "def update_token_range(*args):\n",
        "    token_range_wdg.max = len(tokenizer.encode(text_wdg.value))\n",
        "\n",
        "update_token_range()\n",
        "\n",
        "token_range_wdg.value = [0, token_range_wdg.max]\n",
        "text_wdg.observe(update_token_range, 'value')\n",
        "\n",
        "interact = widgets.interact.options(manual_name='Run Lens', manual=True)\n",
        "\n",
        "plot = interact(\n",
        "    make_plot,\n",
        "    text=text_wdg,\n",
        "    statistic=statistic_wdg,\n",
        "    lens=lens_wdg,\n",
        "    layer_stride=layer_stride_wdg,\n",
        "    token_range=token_range_wdg,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ca27fca65bbd5c56c827a2643e94bc7b2b551ee6ee2fe84566f2c789012bce4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
